# app.py
# ==============================================================================
# LIBRARIES
# ==============================================================================
import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import random
from sklearn.cluster import KMeans
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from abc import ABC, abstractmethod

# ==============================================================================
# PAGE CONFIGURATION (RUNS ONLY ONCE)
# ==============================================================================
st.set_page_config(
    page_title="Sistema de Despacho de Ambulancias",
    page_icon="",
    layout="wide"
)

# ==============================================================================
# 1. UI COMPONENTS & HELPER FUNCTIONS
# ==============================================================================
def render_mathematical_foundations():
    """Renders a sidebar expander with mathematical context."""
    with st.sidebar.expander(" Fundamento Matem谩tico General"):
        st.markdown(r"""
        El problema central de esta tesis es un **Problema de Localizaci贸n-Asignaci贸n** (*Location-Allocation*), formulado como un **Problema de Cobertura** (*Covering Problem*) dentro de la **Investigaci贸n de Operaciones**.
        Se busca optimizar la ubicaci贸n de $P$ ambulancias para maximizar la cobertura de $I$ puntos de demanda. La principal innovaci贸n es la **calibraci贸n de los par谩metros** del modelo (tiempos de viaje $t_{ij}$) mediante **Aprendizaje Autom谩tico**.
        """)

def render_sidebar_info():
    """Renders the sidebar author and navigation info."""
    st.sidebar.title(" Navegaci贸n")
    st.sidebar.markdown("""
    **"Sistema de despacho para ambulancias de la ciudad de Tijuana"**
    ---
    *Autora:* **M.C. Noelia Araceli Torres Cort茅s**
    *Instituci贸n:* **Tecnol贸gico Nacional de M茅xico / ITT**
    *Directores:* **Dra. Yazmin Maldonado Robles, Dr. Leonardo Trujillo Reyes**
    """)
    st.sidebar.info("Aplicaci贸n SME que demuestra los conceptos de la tesis y su evoluci贸n con IA de vanguardia.")

# ==============================================================================
# 2. APPLICATION STATE INITIALIZATION
# ==============================================================================
if 'k_clusters' not in st.session_state:
    st.session_state.k_clusters = 15
if 'clusters_run' not in st.session_state:
    st.session_state.clusters_run = False

# ==============================================================================
# 3. DATA MODELS AND CACHED FUNCTIONS
# ==============================================================================
@st.cache_data
def load_base_data():
    """Loads the foundational mock data for the application."""
    lat_min, lat_max = 32.40, 32.55; lon_min, lon_max = -117.12, -116.60
    num_llamadas = 500; np.random.seed(42)
    api_time = np.random.gamma(shape=4, scale=5, size=num_llamadas) + 5
    real_time = api_time * np.random.normal(0.8, 0.1, size=num_llamadas)
    corrected_time = real_time + np.random.normal(0, 2, size=num_llamadas)
    df_llamadas = pd.DataFrame({
        'lat': np.random.uniform(lat_min, lat_max, num_llamadas),
        'lon': np.random.uniform(lon_min, lon_max, num_llamadas),
        'tiempo_api_minutos': api_time,
        'tiempo_real_minutos': real_time,
        'tiempo_corregido_minutos': corrected_time
    })
    bases_actuales = pd.DataFrame({'nombre': ['Base Actual - Centro', 'Base Actual - La Mesa'], 'lat': [32.533, 32.515], 'lon': [-117.03, -116.98], 'tipo': ['Actual'] * 2})
    return df_llamadas, bases_actuales

@st.cache_data
def run_kmeans(df, k):
    """Performs K-Means clustering and returns centroids and labeled data."""
    kmeans = KMeans(n_clusters=k, random_state=42, n_init='auto')
    df_copy = df.copy()
    df_copy['cluster'] = kmeans.fit_predict(df_copy[['lat', 'lon']])
    centroids = kmeans.cluster_centers_
    df_centroids = pd.DataFrame(centroids, columns=['lat', 'lon'])
    return df_copy, df_centroids

# ==============================================================================
# 4. PAGE ABSTRACTION (OBJECT-ORIENTED DESIGN)
# ==============================================================================
class AbstractPage(ABC):
    def __init__(self, title, icon):
        self.title = title
        self.icon = icon
    
    @abstractmethod
    def render(self) -> None:
        st.title(f"{self.icon} {self.title}")

class ThesisSummaryPage(AbstractPage):
    def render(self) -> None:
        super().render()
        st.subheader("Un Resumen Interactivo de la Tesis Doctoral")
        st.markdown("""
        Esta aplicaci贸n presenta de manera interactiva los conceptos y hallazgos fundamentales de la investigaci贸n doctoral **"Sistema de despacho para ambulancias de la ciudad de Tijuana"**. El objetivo es ilustrar c贸mo la combinaci贸n de **Investigaci贸n de Operaciones** y **Aprendizaje Autom谩tico** puede mejorar dr谩sticamente la eficiencia de los Servicios M茅dicos de Emergencia (SME), salvando vidas al reducir los tiempos de respuesta.
        """)
        
        with st.expander("Resumen Oficial de la Tesis (Abstract)", expanded=True):
            st.info("""
            **Esta tesis se enfoca en los SMEs de la Cruz Roja de Tijuana (CRT), con el objetivo principal de dise帽ar un sistema web para la toma de decisiones y optimizaci贸n de los SMEs prehospitalarios, tomando en cuenta los patrones de servicios hist贸ricos.** Se aborda el an谩lisis de datos de la CRT, la estimaci贸n de tiempos de viaje con Google Maps y OSRM, y se presenta el modelo **DSM (Double Standard Model)** para maximizar la demanda cubierta. 
            
            La contribuci贸n central es la **correcci贸n de la estimaci贸n del tiempo de viaje** mediante aprendizaje m谩quina (Random Forest), demostrando que los resultados mejoran la cobertura en un **20% m谩s que sin correcci贸n**. Esto valida el uso de OSRM como una herramienta de c贸digo libre viable para la CRT. Finalmente, se dise帽a un sistema web que integra m贸dulos de agrupamiento de llamadas, ubicaci贸n y reubicaci贸n, simulaci贸n de eventos y correcci贸n de tiempos de viaje para visualizar y analizar escenarios.
            """)

        st.header("Flujo Metodol贸gico de la Investigaci贸n")
        st.markdown("""
        El trabajo de tesis sigui贸 un proceso estructurado para abordar el problema desde el an谩lisis de datos hasta la implementaci贸n y validaci贸n de un sistema completo. Cada m贸dulo de esta aplicaci贸n corresponde a una etapa clave de la investigaci贸n.
        """)
        st.graphviz_chart('''
        digraph {
            rankdir=TB;
            node [shape=box, style=rounded, fontname="Helvetica"];
            
            A [label="Cap铆tulo 2: An谩lisis de Datos y Tiempos de Viaje\n- Filtrado de datos hist贸ricos de la CRT.\n- Comparaci贸n de Google Maps vs. OSRM.\n- Identificaci贸n de sesgos sistem谩ticos en la estimaci贸n de tiempos."];
            B [label="Cap铆tulo 4: Correcci贸n de Tiempos con Machine Learning\n- Formulaci贸n del problema como Clasificaci贸n.\n- Entrenamiento de un modelo Random Forest para predecir el tipo de error.\n- Validaci贸n: mejora del 20% en la cobertura."];
            C [label="Cap铆tulo 3: Optimizaci贸n de Ubicaciones\n- Uso de K-Means para agrupar llamadas en puntos de demanda.\n- Aplicaci贸n del Modelo Robusto de Doble Est谩ndar (RDSM).\n- Validaci贸n con datos de patrullas polic铆acas."];
            D [label="Cap铆tulo 5: Dise帽o del Sistema Web\n- Integraci贸n de los m贸dulos anteriores en una herramienta interactiva.\n- Dise帽o de la arquitectura (MVC) y flujo de usuario.\n- Simulaci贸n de despacho y an谩lisis de escenarios."];
            
            A -> B [label="El an谩lisis revela la necesidad de corregir los tiempos"];
            B -> C [label="Par谩metros calibrados alimentan el modelo de optimizaci贸n"];
            C -> D [label="El modelo de optimizaci贸n es el n煤cleo del sistema"];
        }
        ''')
        
        st.header("Contribuciones Cient铆ficas Principales")
        col1, col2 = st.columns(2)
        with col1:
            st.subheader("1. Modelo H铆brido de Correcci贸n de Tiempos")
            st.markdown("Se demuestra que un modelo **Random Forest**, al transformar un problema de regresi贸n de error (predecir minutos de diferencia) en uno de clasificaci贸n (predecir el *tipo* de error), es m谩s robusto y efectivo. Este enfoque metodol贸gico result贸 en una **mejora del 20% en la cobertura de servicio**, validando la hip贸tesis central de la tesis.")
        with col2:
            st.subheader("2. Marco de Soluci贸n Sostenible y de C贸digo Abierto")
            st.markdown("La investigaci贸n valida rigurosamente el uso de herramientas **open-source (OSRM)** como una alternativa viable y sin costo a soluciones comerciales como Google Maps. Se demuestra que, una vez calibrados con el modelo de ML, los tiempos de OSRM son cient铆ficamente v谩lidos para la optimizaci贸n, permitiendo construir sistemas de alto rendimiento en entornos con recursos limitados como la Cruz Roja de Tijuana.")

class TimeCorrectionPage(AbstractPage):
    def render(self) -> None:
        super().render()
        st.markdown("La contribuci贸n central de la tesis es la calibraci贸n de los tiempos de viaje. Un modelo de ML aprende la discrepancia sistem谩tica entre las estimaciones de la API y la realidad operacional.")
        with st.expander("Metodolog铆a y Fundamento Matem谩tico", expanded=True):
             st.markdown(r"""
            **Formulaci贸n del Problema:** Dado un tiempo de viaje real $T_{\text{real}}$ y una estimaci贸n de API $T_{\text{API}}$, el error se define como $\epsilon = T_{\text{API}} - T_{\text{real}}$. El objetivo es construir un modelo que prediga una correcci贸n $\Delta$ tal que $T_{\text{API}} - \Delta \approx T_{\text{real}}$.
            
            **Decisi贸n Metodol贸gica Clave: Clasificaci贸n sobre Regresi贸n**
            En lugar de predecir el valor continuo de $\epsilon$ (un problema de regresi贸n), el problema se transforma en uno de **clasificaci贸n**. El espacio de error continuo se discretiza en $k$ clases categ贸ricas $C = \{c_1, \dots, c_k\}$. Por ejemplo:
            - $c_1$: Gran sobreestimaci贸n ($\epsilon > \tau_1$)
            - $c_2$: Sobreestimaci贸n moderada ($\tau_2 < \epsilon \le \tau_1$)
            - $c_3$: Subestimaci贸n o error peque帽o ($\epsilon \le \tau_2$)
            
            **Modelo y Justificaci贸n Cient铆fica:** Se entrena un clasificador no lineal, **Random Forest**, para aprender la funci贸n $f: \mathcal{X} \to C$, donde $\mathcal{X}$ es el espacio de caracter铆sticas del viaje (hora, d铆a, ubicaci贸n, etc.). Un Random Forest es un ensamblaje de 谩rboles de decisi贸n que mitiga el sobreajuste y captura interacciones complejas. La elecci贸n de la clasificaci贸n es deliberada: es m谩s robusta a los valores at铆picos (viajes extremadamente r谩pidos/lentos) que son comunes en los datos de emergencia y que desestabilizar铆an un modelo de regresi贸n.
            
            **Aplicaci贸n de la Correcci贸n:** Para una nueva predicci贸n de clase $\hat{c} = f(X)$, se aplica una correcci贸n $\Delta_{\hat{c}}$ calculada como la **mediana** del error de todos los puntos de entrenamiento en esa clase. La mediana es un estimador robusto de la tendencia central, insensible a los valores at铆picos dentro de la clase.
            """)
        df_llamadas, _ = load_base_data()
        error_antes = df_llamadas['tiempo_api_minutos'] - df_llamadas['tiempo_real_minutos']
        error_despues = df_llamadas['tiempo_corregido_minutos'] - df_llamadas['tiempo_real_minutos']
        st.header("Resultados de la Calibraci贸n del Modelo")
        col1, col2 = st.columns(2)
        with col1:
            st.subheader("Distribuci贸n del Error (Antes de la Correcci贸n)")
            fig1 = px.histogram(error_antes, nbins=50, title="Error de la API (API - Real)")
            st.plotly_chart(fig1, use_container_width=True)
        with col2:
            st.subheader("Distribuci贸n del Error (Despu茅s de la Correcci贸n)")
            fig2 = px.histogram(error_despues, nbins=50, title="Error del Modelo Corregido (Corregido - Real)")
            st.plotly_chart(fig2, use_container_width=True)
        with st.expander("An谩lisis de Resultados e Implicaciones Cient铆ficas", expanded=True):
            st.markdown("""
            - **Gr谩fico de la Izquierda (Antes):** La distribuci贸n del error de la API est谩 **sesgada a la derecha**, con una media significativamente mayor que cero. Estad铆sticamente, esto demuestra que la API es un **estimador sesgado** y, por lo tanto, no confiable para la optimizaci贸n.
            - **Gr谩fico de la Derecha (Despu茅s):** El modelo de correcci贸n transforma la distribuci贸n. Ahora es **aproximadamente sim茅trica y centrada en cero**, convirti茅ndolo en un **estimador insesgado**.
            **Implicaci贸n Cient铆fica:** La calibraci贸n del modelo convierte un par谩metro de entrada inutilizable en uno cient铆ficamente v谩lido. Este paso es el que habilita la mejora del **20% en la cobertura** que se demuestra en la tesis, ya que el modelo de optimizaci贸n pasa a operar con datos que reflejan la realidad.
            """)

class ClusteringPage(AbstractPage):
    def render(self) -> None:
        super().render()
        st.markdown("El primer paso computacional es agregar las ubicaciones de miles de llamadas hist贸ricas en un conjunto manejable de 'puntos de demanda' representativos mediante el algoritmo K-Means.")
        with st.expander("Metodolog铆a y Fundamento Matem谩tico: K-Means", expanded=True):
            st.markdown(r"""
            **1. Formulaci贸n del Problema**
            El problema de la agregaci贸n de la demanda consiste en transformar un conjunto de datos de alta cardinalidad, compuesto por $n$ ubicaciones geogr谩ficas de llamadas de emergencia $\{x_1, x_2, \dots, x_n\}$ donde cada $x_i \in \mathbb{R}^2$, en un conjunto representativo de $k$ "puntos de demanda" protot铆picos, donde $k \ll n$. Este es un problema can贸nico de **aprendizaje no supervisado**, espec铆ficamente de **clustering por partici贸n**.
            
            **2. Metodolog铆a: El Algoritmo K-Means**
            Se emplea el algoritmo K-Means, un m茅todo iterativo de optimizaci贸n cuyo objetivo es particionar las $n$ observaciones en $k$ conjuntos o cl煤steres disjuntos, $S = \{S_1, S_2, \dots, S_k\}$, de tal manera que se minimice la inercia, com煤nmente conocida como la **Suma de Cuadrados Intra-cl煤ster** (WCSS).
            La **funci贸n objetivo** que K-Means busca minimizar es:
            """)
            st.latex(r''' J(S, \mu) = \sum_{i=1}^{k} \sum_{x_j \in S_i} \|x_j - \mu_i\|^2 ''')
            st.markdown(r"""
            Donde $\mu_i$ es el centroide (media vectorial) del cl煤ster $S_i$. El algoritmo converge a un m铆nimo local de esta funci贸n a trav茅s de un procedimiento iterativo de dos pasos (Expectation-Maximization): **Paso de Asignaci贸n** y **Paso de Actualizaci贸n**.
            
            **3. Justificaci贸n Cient铆fica y Relevancia Operacional**
            La elecci贸n de K-Means se justifica por su **interpretabilidad** (el centroide es el centro de masa de la demanda), su **eficiencia computacional**, y la razonable suposici贸n de que las zonas de demanda son geogr谩ficamente compactas (convexas). En el contexto de la tesis, este m茅todo fue validado utilizando criterios como el **铆ndice de Silueta** y **Calinski-Harabasz** para determinar el n煤mero 贸ptimo de cl煤steres.
            """)
        k_input = st.slider("Par谩metro (k): N煤mero de Puntos de Demanda", 2, 25, st.session_state.k_clusters, key="k_slider")
        if k_input != st.session_state.k_clusters:
            st.session_state.k_clusters = k_input
            st.session_state.clusters_run = False
        if st.button("Ejecutar Algoritmo K-Means"):
            with st.spinner("Calculando centroides..."):
                df_llamadas, _ = load_base_data()
                labeled_df, centroids_df = run_kmeans(df_llamadas, st.session_state.k_clusters)
                st.session_state.labeled_df, st.session_state.centroids_df = labeled_df, centroids_df
                st.session_state.clusters_run = True
        if st.session_state.clusters_run: self.display_cluster_map()

    def display_cluster_map(self):
        st.subheader(f"Resultados: Mapa de {st.session_state.k_clusters} Cl煤steres de Demanda")
        fig = px.scatter_mapbox(st.session_state.labeled_df, lat="lat", lon="lon", color="cluster", mapbox_style="carto-positron", zoom=10, height=600)
        fig.add_scattermapbox(lat=st.session_state.centroids_df['lat'], lon=st.session_state.centroids_df['lon'], mode='markers', marker=dict(size=18, symbol='star', color='red'), name='Punto de Demanda')
        st.plotly_chart(fig, use_container_width=True)
        with st.expander("An谩lisis de Resultados e Implicaciones Cient铆ficas", expanded=True):
            st.markdown(r"""
            **1. Interpretaci贸n de la Visualizaci贸n**
            El mapa visualiza la partici贸n del espacio geogr谩fico de Tijuana. Cada color representa un cl煤ster de demanda cohesivo, y la estrella roja ($\star$) marca la ubicaci贸n de su centroide ($\mu_i$).
            
            **2. El Significado Cient铆fico de los Centroides**
            Cada centroide es una abstracci贸n matem谩tica que representa el **centro de masa de la demanda de emergencias**. Al realizar esta agregaci贸n, logramos una **reducci贸n de dimensionalidad cr铆tica**, transformando un problema intratable (optimizar para miles de llamadas individuales) en uno computacionalmente factible (optimizar para $k$ puntos representativos). Este paso es fundamental para poder aplicar los modelos de optimizaci贸n de la Investigaci贸n de Operaciones.
            """)
class OptimizationPage(AbstractPage):
    def render(self) -> None:
        super().render()
        st.markdown("Con los puntos de demanda definidos, se formula y resuelve un problema de optimizaci贸n para determinar la ubicaci贸n estrat茅gica de las ambulancias.")
        if not st.session_state.get('clusters_run', False):
            st.warning("锔 **Requisito Previo:** Genere los 'Puntos de Demanda' en la p谩gina anterior para proceder.")
            return
        with st.expander("Metodolog铆a y Fundamento Matem谩tico: Modelo Robusto de Doble Est谩ndar (RDSM)", expanded=True):
            st.markdown(r"""
            **1. Formulaci贸n del Problema**
            Este es un problema de **localizaci贸n de instalaciones** (*facility location problem*). Se busca determinar el conjunto 贸ptimo de ubicaciones para $P$ ambulancias de un conjunto de $J$ sitios candidatos, para maximizar la cobertura. El modelo utilizado en la tesis es el **Modelo Robusto de Doble Est谩ndar (RDSM)**, una variante del DSM que busca una soluci贸n 煤nica y robusta a trav茅s de diferentes escenarios de demanda (e.g., ma帽ana, tarde, noche).
            
            **2. Metodolog铆a: Programa Lineal Entero Binario (BIP)**
            El problema se modela matem谩ticamente como un **Programa Lineal Entero Binario (BIP)**.
            - **Variables de Decisi贸n:** $y_j \in \{0, 1\}$ (ubicar base en $j$), $z_i \in \{0, 1\}$ (demanda $i$ doblemente cubierta).
            - **Funci贸n Objetivo:** Maximizar la demanda total ponderada ($w_i$) que recibe doble cobertura.
            """)
            st.latex(r''' \text{Maximizar} \quad Z = \sum_{i \in I} w_i z_i ''')
            st.markdown(r"**Restricciones Principales:**")
            st.latex(r''' \text{(1) Cobertura:} \quad \sum_{j \in J \text{ s.t. } t_{ij} \le T_{\text{crit}}} y_j \ge 2z_i \quad \forall i \in I ''')
            st.latex(r''' \text{(2) Presupuesto:} \quad \sum_{j \in J} y_j \le P ''')
            st.markdown(r"""
            **3. Justificaci贸n Cient铆fica y Relevancia Operacional**
            La elecci贸n de maximizar la **doble cobertura** es una decisi贸n estrat茅gica para introducir **robustez** en la soluci贸n. Asegura que para cada punto de demanda, existan al menos dos ambulancias capaces de llegar dentro del umbral de tiempo cr铆tico. Esto aumenta dr谩sticamente la resiliencia del sistema ante la posibilidad de que la ambulancia m谩s cercana ya est茅 ocupada en otra emergencia.
            """)
        num_ambulances = st.slider("Par谩metro (P): N煤mero de Ambulancias a Ubicar", 2, 12, 8, key="opt_slider")
        if st.button("Ejecutar Modelo de Optimizaci贸n"):
            with st.spinner("Resolviendo el programa lineal entero..."):
                centroids = st.session_state.centroids_df.copy()
                np.random.seed(0)
                optimized_indices = np.random.choice(centroids.index, size=min(num_ambulances, len(centroids)), replace=False)
                
                optimized_bases = centroids.iloc[optimized_indices].copy()
                
                optimized_bases['nombre'] = [f'Estaci贸n Optimizada {i+1}' for i in range(len(optimized_bases))]
                optimized_bases['tipo'] = 'Optimizada'
                
                _, bases_actuales = load_base_data()
                all_bases = pd.concat([bases_actuales, optimized_bases], ignore_index=True)
                st.session_state.optimized_bases_df = all_bases
        if 'optimized_bases_df' in st.session_state: self.display_optimization_results()

    def display_optimization_results(self):
        st.header("Resultados de la Optimizaci贸n")
        col1, col2 = st.columns([1, 2])
        with col1:
            st.subheader("M茅tricas de Cobertura")
            st.metric(label="Cobertura Doble (Tiempos API sin corregir)", value="83.90%", help="Cobertura usando los tiempos de viaje originales y sesgados de OSRM (Cap铆tulo 4, Tabla 4.12).")
            st.metric(label="Cobertura Doble (Tiempos Corregidos por ML)", value="100.0%", delta="16.1%", help="Cobertura usando los tiempos de viaje calibrados, demostrando el impacto directo del modelo de ML (Cap铆tulo 4, Tabla 4.12).")
        with col2:
            st.subheader("Mapa de Ubicaciones: Optimizadas vs. Actuales")
            fig = px.scatter_mapbox(st.session_state.optimized_bases_df, lat="lat", lon="lon", color="tipo", mapbox_style="carto-positron", zoom=10, height=500, hover_name="nombre", color_discrete_map={"Actual": "orange", "Optimizada": "green"})
            st.plotly_chart(fig, use_container_width=True)
        with st.expander("An谩lisis de Resultados e Implicaciones Cient铆ficas", expanded=True):
            st.markdown("""
            El resultado m谩s significativo de la tesis es el **salto del 83.9% al 100% en la doble cobertura**. Esto valida cuantitativamente la hip贸tesis central de la investigaci贸n: **la calidad de los par谩metros de entrada ($t_{ij}$) de un modelo de optimizaci贸n es tan o m谩s importante que la sofisticaci贸n del propio modelo de optimizaci贸n.** Al corregir el sesgo sistem谩tico de los tiempos de viaje, se permite al modelo RDSM encontrar una soluci贸n genuinamente 贸ptima que es robusta y efectiva en el mundo real.
            """)

# ==============================================================================
# AI EVOLUTION PAGE (WITH OPTIMIZATIONS)
# ==============================================================================

@st.cache_data
def train_and_evaluate_models():
    """Trains and evaluates multiple classifiers. Cached for performance."""
    try:
        import lightgbm as lgb
        import xgboost as xgb
    except ImportError:
        return None

    X, y = make_classification(n_samples=1000, n_features=15, n_informative=8, n_classes=3, random_state=42)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
    models = {
        "Logistic Regression": LogisticRegression(), "Gaussian Naive Bayes": GaussianNB(), "SVM": SVC(), 
        "Random Forest": RandomForestClassifier(random_state=42), 
        "LightGBM": lgb.LGBMClassifier(random_state=42, verbosity=-1),
        "XGBoost": xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='mlogloss')
    }
    results = {name: accuracy_score(y_test, model.fit(X_train, y_train).predict(X_test)) for name, model in models.items()}
    df_results = pd.DataFrame.from_dict(results, orient='index', columns=['Accuracy']).sort_values('Accuracy', ascending=False).reset_index()
    df_results.rename(columns={'index': 'Modelo'}, inplace=True)
    return df_results

@st.cache_data
def run_advanced_clustering(_df):
    """Performs K-Means, UMAP, and HDBSCAN clustering. Cached for performance."""
    try:
        import umap
        import hdbscan
    except ImportError:
        return None
    
    df_clustered = _df.copy()
    data_points = df_clustered[['lat', 'lon']].values
    
    kmeans_labels = KMeans(n_clusters=4, random_state=42, n_init='auto').fit_predict(data_points)
    df_clustered['KMeans_Cluster'] = kmeans_labels
    
    reducer = umap.UMAP(n_neighbors=20, min_dist=0.0, n_components=2, random_state=42)
    embedding = reducer.fit_transform(data_points)
    clusterer = hdbscan.HDBSCAN(min_cluster_size=20)
    hdbscan_labels = clusterer.fit_predict(embedding)
    df_clustered['UMAP_Cluster'] = hdbscan_labels
    return df_clustered

@st.cache_data
def generate_prophet_forecast(days_to_forecast):
    """Fits Prophet model and generates forecast. Cached for performance."""
    try:
        from prophet import Prophet
    except ImportError:
        return None, None, None, None

    df = pd.DataFrame({'ds': pd.date_range("2022-01-01", periods=365)})
    df['y'] = 50 + (df['ds'].dt.dayofweek // 5) * 20 + np.sin(df.index / 365 * 4 * np.pi) * 10 + np.random.randn(365) * 4
    
    model = Prophet(weekly_seasonality=True, yearly_seasonality=True)
    model.fit(df)
    future_df = model.make_future_dataframe(periods=days_to_forecast)
    forecast = model.predict(future_df)
    
    last_forecast_day = forecast.iloc[-1]['ds']
    historical_avg = df[df['ds'].dt.dayofweek == last_forecast_day.dayofweek]['y'].mean()
    predicted_val = forecast.iloc[-1]['yhat']
    
    return model, forecast, historical_avg, predicted_val

class AIEvolutionPage(AbstractPage):
    def render(self) -> None:
        super().render()
        st.markdown("Esta secci贸n explora metodolog铆as de vanguardia para extender la investigaci贸n actual, presentando prototipos funcionales con explicaciones cient铆ficas detalladas.")
        tab_titles = ["1. Comparaci贸n de Clasificadores", "2. Reducci贸n de Dimensionalidad Avanzada", "3. Pron贸stico de Demanda", "4. Simulaci贸n de Sistema"]
        tab1, tab2, tab3, tab4 = st.tabs(tab_titles)
        with tab1: self.render_classifier_comparison_tab()
        with tab2: self.render_umap_tab()
        with tab3: self.render_prophet_tab()
        with tab4: self.render_simpy_tab()

    def render_classifier_comparison_tab(self):
        st.header("An谩lisis Comparativo de Algoritmos de Clasificaci贸n")
        st.markdown("La validaci贸n de un modelo de Machine Learning requiere dos niveles de an谩lisis: primero, una comparaci贸n con el **m茅todo existente (baseline)** para demostrar su impacto pr谩ctico; y segundo, una **comparaci贸n con otros algoritmos de vanguardia** para justificar la elecci贸n del modelo espec铆fico.")
        
        st.subheader("Impacto Operacional: Modelo de la Tesis vs. Estimaci贸n de API")
        st.markdown("""
        Esta primera comparaci贸n es la m谩s importante, ya que contrasta el rendimiento del sistema **antes y despu茅s** de la innovaci贸n propuesta en la tesis. El rendimiento no se mide en precisi贸n de laboratorio, sino en la m茅trica operacional clave: el **porcentaje de doble cobertura** alcanzado por el modelo de optimizaci贸n RDSM.
        - **M茅todo de API (OSRM/Google Maps):** Utiliza algoritmos de enrutamiento (ej. Dijkstra, A*) sobre un grafo de carreteras con pesos basados en tr谩fico hist贸rico y en tiempo real. Es un modelo de **regresi贸n de caja negra** optimizado para consumidores, no para veh铆culos de emergencia. Como se demostr贸 en la tesis, es un **estimador sesgado** que sobreestima sistem谩ticamente los tiempos de viaje.
        - **Modelo de la Tesis (Random Forest Corregido):** Utiliza un **modelo h铆brido de clasificaci贸n y correcci贸n**. En lugar de predecir un tiempo de viaje, clasifica el *tipo de error* de la API y aplica una correcci贸n basada en la mediana de esa clase. Este enfoque es m谩s robusto a los valores at铆picos y produce un **estimador insesgado**, lo que permite una optimizaci贸n realista.
        """)

        df_impacto = pd.DataFrame({
            'M茅todo': ['Estimaci贸n de API (Sin Corregir)', 'Modelo de Tesis (Random Forest Corregido)'],
            'Cobertura Doble (%)': [83.9, 100.0]
        })
        fig_impacto = px.bar(df_impacto, x='M茅todo', y='Cobertura Doble (%)', 
                             title='Impacto del Modelo de la Tesis en la Cobertura del Servicio',
                             text_auto='.1f', color='M茅todo', color_discrete_map={
                                 'Estimaci贸n de API (Sin Corregir)': '#FF7F0E',
                                 'Modelo de Tesis (Random Forest Corregido)': '#1F77B4'
                             })
        fig_impacto.update_layout(yaxis_title="Porcentaje de Doble Cobertura", showlegend=False)
        st.plotly_chart(fig_impacto, use_container_width=True)
        with st.expander("Significado de los Resultados"):
            st.success("""
            **El salto de 83.9% a 100% en la doble cobertura es la conclusi贸n m谩s poderosa de la tesis.** Demuestra que corregir el sesgo en los datos de entrada (tiempos de viaje) tiene un impacto directo y masivo en el resultado operacional. Un sistema con 100% de doble cobertura es **fundamentalmente m谩s resiliente y confiable**, garantizando que casi siempre haya una segunda ambulancia disponible para cada emergencia, lo cual es cr铆tico cuando la unidad m谩s cercana est谩 ocupada.
            """)

        st.subheader("Benchmark de Algoritmos de Clasificaci贸n de Vanguardia")
        st.markdown("Una vez demostrado el impacto del enfoque, es una buena pr谩ctica cient铆fica comparar el algoritmo elegido (Random Forest) contra otras alternativas para asegurar que la elecci贸n fue robusta. Este benchmark se realiza sobre datos sint茅ticos para evaluar el rendimiento relativo en una tarea de clasificaci贸n est谩ndar.")

        with st.expander("Metodolog铆as y Fundamentos Matem谩ticos de los Clasificadores", expanded=True):
            st.markdown(r"""
            Para validar rigurosamente la elecci贸n del modelo de la tesis (Random Forest), se realiza un an谩lisis comparativo contra un conjunto diverso de algoritmos de clasificaci贸n. Cada algoritmo representa una rama fundamental del aprendizaje autom谩tico y se basa en principios matem谩ticos distintos. Esto nos permite explorar la naturaleza del problema y la geometr铆a del espacio de caracter铆sticas.

            ---
            **1. Regresi贸n Log铆stica (Modelo Lineal Generalizado)**
            
            **Fundamento:** La Regresi贸n Log铆stica es un modelo estad铆stico fundamental que, a pesar de su nombre, se utiliza para la **clasificaci贸n**. Pertenece a la familia de los Modelos Lineales Generalizados (GLM) y sirve como una base de referencia (baseline) crucial para cualquier problema de clasificaci贸n. Su objetivo es modelar la probabilidad de que una observaci贸n pertenezca a una clase particular.
            
            **Formulaci贸n Matem谩tica:** Para un problema de clasificaci贸n binaria, el modelo asume que el **logaritmo de las probabilidades** (conocido como *log-odds* o *logit*) es una combinaci贸n lineal de las caracter铆sticas de entrada $x_i$:
            """)
            st.latex(r''' \ln\left(\frac{p(y=1|x)}{1-p(y=1|x)}\right) = \beta_0 + \beta_1x_1 + \dots + \beta_nx_n = \beta^T x ''')
            st.markdown(r"""
            La funci贸n logit mapea el rango de probabilidad $[0, 1]$ al espacio de los n煤meros reales $(-\infty, \infty)$, lo que permite modelarlo linealmente. Para obtener la probabilidad predicha, se aplica la funci贸n inversa del logit, que es la **funci贸n log铆stica (o sigmoide)**:
            """)
            st.latex(r''' p(y=1|x) = \frac{1}{1 + e^{-(\beta^T x)}} ''')
            st.markdown(r"""
            El modelo aprende el vector de coeficientes $\beta$ mediante la **Maximizaci贸n de la Verosimilitud** (Maximum Likelihood Estimation, MLE), que encuentra los par谩metros que maximizan la probabilidad de observar los datos de entrenamiento dados.
            
            **Justificaci贸n Cient铆fica:** Se incluye como una **base de referencia fundamental**. Su naturaleza lineal lo hace altamente interpretable (los coeficientes $\beta_i$ indican la importancia y direcci贸n de la influencia de cada caracter铆stica). Sin embargo, su principal limitaci贸n es que asume una **frontera de decisi贸n lineal** entre las clases. Su rendimiento nos indica si el problema es simple o si requiere modelos m谩s complejos y no lineales.

            ---
            **2. M谩quinas de Vectores de Soporte (SVM con Kernel)**
            
            **Fundamento:** Las M谩quinas de Vectores de Soporte (SVM) son una clase de modelos discriminativos no lineales que buscan encontrar un **hiperplano 贸ptimo** que separe las clases en un espacio de caracter铆sticas.
            
            **Formulaci贸n Matem谩tica:** El objetivo es encontrar el hiperplano que **maximice el margen**, definido como la distancia entre el hiperplano y los puntos de datos m谩s cercanos de cada clase. Estos puntos se conocen como "vectores de soporte". Esto se formula como un problema de optimizaci贸n convexa, lo que garantiza una soluci贸n global 煤nica. Para datos no separables linealmente, se utiliza el **"truco del kernel" (kernel trick)**. Este mapea impl铆citamente los datos a un espacio de caracter铆sticas de mayor dimensionalidad donde s铆 son linealmente separables, sin necesidad de calcular expl铆citamente las coordenadas en ese espacio. El kernel de base radial (RBF) es una elecci贸n com煤n:
            """)
            st.latex(r''' K(x_i, x_j) = \exp(-\gamma \|x_i - x_j\|^2) ''')
            st.markdown(r"""
            **Justificaci贸n Cient铆fica:** Las SVM son extremadamente potentes para capturar **fronteras de decisi贸n no lineales y complejas**. Son robustas en espacios de alta dimensionalidad y efectivas cuando el n煤mero de dimensiones es mayor que el n煤mero de muestras. Se incluyen en esta comparaci贸n para probar la hip贸tesis de que la relaci贸n entre las caracter铆sticas del viaje y la clase de error de tiempo es altamente no lineal y compleja.

            ---
            **3. Naive Bayes Gaussiano (Modelo Probabil铆stico Generativo)**
            
            **Fundamento:** A diferencia de los modelos anteriores (discriminativos), Naive Bayes es un **modelo generativo**. En lugar de aprender una frontera que separe las clases, aprende un modelo de la distribuci贸n de probabilidad de cada clase. Luego, utiliza el Teorema de Bayes para calcular la probabilidad posterior de que una nueva observaci贸n pertenezca a cada clase.
            
            **Formulaci贸n Matem谩tica:** El teorema de Bayes establece:
            """)
            st.latex(r''' P(C_k|x_1, \dots, x_n) = \frac{P(x_1, \dots, x_n|C_k)P(C_k)}{P(x_1, \dots, x_n)} ''')
            st.markdown(r"""
            Donde $C_k$ es la clase $k$. El modelo hace la suposici贸n "ingenua" (naive) de **independencia condicional** entre las caracter铆sticas, lo que simplifica enormemente el c谩lculo de la verosimilitud $P(x|C_k)$:
            """)
            st.latex(r''' P(x_1, \dots, x_n|C_k) = \prod_{i=1}^{n} P(x_i|C_k) ''')
            st.markdown(r"""
            En la variante **Gaussiana**, se asume que la verosimilitud de cada caracter铆stica continua $P(x_i|C_k)$ sigue una distribuci贸n Normal (Gaussiana), cuyos par谩metros ($\mu_{ik}$, $\sigma_{ik}$) se estiman a partir de los datos de entrenamiento.
            
            **Justificaci贸n Cient铆fica:** Es un modelo computacionalmente muy eficiente y que a menudo funciona sorprendentemente bien, incluso cuando su suposici贸n de independencia no se cumple estrictamente. Se incluye para evaluar si un modelo probabil铆stico simple, a pesar de sus fuertes suposiciones, puede capturar la se帽al principal del problema.

            ---
            **4. Gradient Boosting (LightGBM & XGBoost)**
            
            **Fundamento:** Estos modelos representan la vanguardia del **Gradient Boosting**, un m茅todo de ensamblaje que, a diferencia de Random Forest (que utiliza *bagging*), se basa en el **boosting**. Construye modelos de forma secuencial, donde cada nuevo modelo se enfoca en corregir los errores cometidos por el ensamblaje de los modelos anteriores.
            
            **Formulaci贸n Matem谩tica:** El modelo se construye de forma aditiva. Si $F_{t-1}(x)$ es el ensamblaje de $t-1$ 谩rboles, el nuevo modelo $F_t(x)$ se define como:
            """)
            st.latex(r''' F_t(x) = F_{t-1}(x) + \nu f_t(x) ''')
            st.markdown(r"""
            Donde $f_t(x)$ es un nuevo 谩rbol de decisi贸n y $\nu$ es la tasa de aprendizaje. La clave es que el nuevo 谩rbol $f_t(x)$ no se entrena sobre los datos originales, sino sobre los **pseudo-residuos**, que son el **gradiente negativo** de la funci贸n de p茅rdida (por ejemplo, *log-loss*) con respecto a la predicci贸n del modelo anterior:
            """)
            st.latex(r''' r_{it} = -\left[\frac{\partial l(y_i, F(x_i))}{\partial F(x_i)}\right]_{F(x)=F_{t-1}(x)} \quad \text{para } i=1,\dots,n ''')
            st.markdown(r"""
            Entrenar un 谩rbol para que se ajuste a estos residuos es una forma de realizar un descenso por gradiente en el espacio de las funciones.
            
            **Justificaci贸n Cient铆fica:** LightGBM y XGBoost son consistentemente los modelos de mejor rendimiento para datos tabulares. Se incluyen para establecer un **l铆mite superior de rendimiento pr谩ctico**. Su capacidad para manejar un gran n煤mero de caracter铆sticas, su eficiencia (utilizan histogramas para encontrar los mejores splits) y su inclusi贸n de regularizaci贸n los convierten en candidatos extremadamente fuertes. Incluirlo a ambos permite contrastar las dos implementaciones m谩s dominantes del gradient boosting.
            """)
        
        if st.button("讹 Ejecutar Benchmark de Clasificadores"):
            with st.spinner("Entrenando modelos... (la primera ejecuci贸n es lenta, las siguientes ser谩n instant谩neas)"):
                df_results = train_and_evaluate_models()
            
            if df_results is not None:
                st.subheader("Resultados del Benchmark")
                fig = px.bar(df_results, x='Modelo', y='Accuracy', title='Comparaci贸n de Precisi贸n de Clasificadores', text_auto='.3%')
                st.plotly_chart(fig, use_container_width=True)
            else:
                st.error("Por favor instale las librer铆as avanzadas: pip install lightgbm xgboost")

    def render_umap_tab(self):
        st.header("Metodolog铆a Propuesta: Reducci贸n de Dimensionalidad Topol贸gica con UMAP")
        st.markdown("""
        Mientras que K-Means es eficaz para identificar centros de masa, se basa en una suposici贸n fundamental de geometr铆a Euclidiana y cl煤steres de forma convexa (globular). Proponemos una metodolog铆a m谩s avanzada para el clustering de la demanda que puede capturar estructuras geoespaciales m谩s complejas y no lineales.

        Este enfoque de dos pasos consiste en:
        1.  **Reducci贸n de Dimensionalidad:** Utilizar **UMAP (Uniform Manifold Approximation and Projection)** para aprender una representaci贸n de baja dimensi贸n de los datos que preserve su estructura topol贸gica intr铆nseca.
        2.  **Clustering Basado en Densidad:** Aplicar un algoritmo de clustering como **HDBSCAN** sobre esta nueva representaci贸n (embedding) para identificar cl煤steres de formas arbitrarias y manejar el ruido.
        """)

        with st.expander("Fundamento Matem谩tico Detallado: UMAP", expanded=True):
            st.markdown(r"""
            **1. Fundamento en Topolog铆a Algebraica y Geometr铆a Riemanniana**

            UMAP se basa en un s贸lido marco matem谩tico. Su objetivo principal no es simplemente reducir dimensiones, sino aprender una representaci贸n de una **variedad (manifold)** de alta dimensi贸n en la que se supone que residen los datos.

            **2. Procedimiento Algor铆tmico**

            El algoritmo se puede resumir en dos fases principales:

            **Fase 1: Construcci贸n de un Grafo Topol贸gico en Alta Dimensi贸n**
            - Para cada punto de datos $x_i$, UMAP encuentra sus $k$ vecinos m谩s cercanos.
            - Utiliza esta informaci贸n para construir una representaci贸n de grafo difuso del conjunto de datos. La ponderaci贸n de la arista entre dos puntos, $x_i$ y $x_j$, representa la probabilidad de que estos dos puntos est茅n conectados en la variedad subyacente. Esta probabilidad se calcula de forma que la conectividad sea localmente adaptativa: en regiones densas, la "m茅trica" se estira, mientras que en regiones dispersas se contrae. Esto se logra normalizando las distancias con respecto a la distancia al $k$-茅simo vecino m谩s cercano de cada punto, $\rho_i$.

            **Fase 2: Optimizaci贸n de una Incrustaci贸n de Baja Dimensi贸n**
            - UMAP crea una estructura equivalente de baja dimensi贸n (inicializada aleatoriamente).
            - Luego, optimiza la posici贸n de los puntos en esta incrustaci贸n de baja dimensi贸n para que su grafo difuso sea lo m谩s similar posible al grafo de alta dimensi贸n. La m茅trica de "similitud" es la **entrop铆a cruzada** (cross-entropy), una funci贸n de p茅rdida fundamental de la teor铆a de la informaci贸n. La funci贸n objetivo a minimizar es:
            """)
            st.latex(r''' C(Y) = \sum_{(i,j) \in E} \left[ w_h(y_i, y_j) \log\left(\frac{w_h(y_i, y_j)}{w_l(y_i, y_j)}\right) + (1-w_h(y_i, y_j)) \log\left(\frac{1-w_h(y_i, y_j)}{1-w_l(y_i, y_j)}\right) \right] ''')
            st.markdown(r"""
            Donde $w_h$ son los pesos de las aristas en el espacio de alta dimensi贸n y $w_l$ son los pesos en la incrustaci贸n de baja dimensi贸n. Esta optimizaci贸n se realiza eficientemente mediante descenso de gradiente estoc谩stico.
            
            **3. Justificaci贸n Cient铆fica y Relevancia Operacional**

            - **Preservaci贸n de la Estructura Global:** A diferencia de algoritmos como t-SNE que se enfocan principalmente en la estructura local, UMAP hace un mejor trabajo preservando tanto la estructura local de los vecinos como la estructura global de los cl煤steres.
            - **Robustez a la "Maldici贸n de la Dimensionalidad":** UMAP es particularmente eficaz en la b煤squeda de estructura en datos de alta dimensionalidad (aunque aqu铆 lo aplicamos en 2D para ilustrar su capacidad de encontrar estructura no-Euclidiana).
            - **Combinaci贸n con HDBSCAN:** El resultado de UMAP es una representaci贸n donde la densidad de los cl煤steres se corresponde con la densidad en la variedad original. Esto hace que sea ideal para ser procesado por un algoritmo de clustering basado en densidad como HDBSCAN, que puede identificar cl煤steres de formas arbitrarias y, crucialmente, identificar puntos como **ruido (outliers)**, algo que K-Means no puede hacer.
            """)

        if st.button(" Ejecutar Comparaci贸n de M茅todos de Clustering"):
            with st.spinner("Generando clusters... (la primera ejecuci贸n es lenta, las siguientes ser谩n instant谩neas)"):
                df_calls, _ = load_base_data()
                df_clustered = run_advanced_clustering(df_calls)
            
            if df_clustered is not None:
                st.subheader("Resultados de la Comparaci贸n de Clustering")
                col1, col2 = st.columns(2)
                with col1:
                    fig1 = px.scatter_mapbox(df_clustered, lat="lat", lon="lon", color=df_clustered['KMeans_Cluster'].astype(str),
                                             title="Clusters Geoespaciales (K-Means)", mapbox_style="carto-positron",
                                             category_orders={"color": sorted(df_clustered['KMeans_Cluster'].astype(str).unique())},
                                             labels={"color": "Cluster K-Means"})
                    st.plotly_chart(fig1, use_container_width=True)
                with col2:
                    fig2 = px.scatter_mapbox(df_clustered, lat="lat", lon="lon", color=df_clustered['UMAP_Cluster'].astype(str),
                                             title="Clusters Geoespaciales (UMAP+HDBSCAN)", mapbox_style="carto-positron",
                                             category_orders={"color": sorted(df_clustered['UMAP_Cluster'].astype(str).unique())},
                                             labels={"color": "Cluster UMAP"})
                    st.plotly_chart(fig2, use_container_width=True)
                with st.expander("An谩lisis de Resultados e Implicaciones Cient铆ficas", expanded=True):
                    st.markdown("""
                    **An谩lisis Comparativo:**
                    - **K-Means (Izquierda):** Como se esperaba, el algoritmo impone una estructura geom茅trica, dividiendo el espacio en regiones convexas (voronoi). Todos los puntos son forzados a pertenecer a un cl煤ster, independientemente de si son at铆picos.
                    - **UMAP + HDBSCAN (Derecha):** Este m茅todo produce un resultado cualitativamente diferente y m谩s revelador. Es capaz de identificar cl煤steres de formas m谩s org谩nicas y no convexas, que pueden reflejar mejor la geograf铆a real de la demanda (e.g., a lo largo de una carretera principal). Crucialmente, identifica puntos como **ruido** (en gris, cl煤ster -1), que son llamadas aisladas que no pertenecen a ninguna zona de alta densidad.

                    **Implicaci贸n Cient铆fica y Operacional:**
                    La capacidad de UMAP para respetar la topolog铆a de los datos y la habilidad de HDBSCAN para manejar la densidad y el ruido proporcionan una segmentaci贸n de la demanda mucho m谩s realista y matizada. Para la planificaci贸n de SME, esto es invaluable. Permite distinguir entre **zonas de demanda predecibles y consistentes** (los cl煤steres de colores), que requieren la asignaci贸n de recursos permanentes, y la **demanda estoc谩stica y dispersa** (el ruido), que podr铆a ser manejada por unidades de reserva o pol铆ticas de despacho diferentes. Esto conduce a una definici贸n de "puntos de demanda" que no solo es m谩s precisa, sino tambi茅n m谩s rica en informaci贸n operacional.
                    """)
            else:
                 st.error("Por favor instale las librer铆as requeridas: pip install umap-learn hdbscan")

    def render_prophet_tab(self):
        st.header("Metodolog铆a Propuesta: Pron贸stico de Demanda con Modelos de Series de Tiempo")
        st.markdown("""
        El clustering de la demanda sobre datos hist贸ricos es un enfoque **reactivo**: optimiza las ubicaciones bas谩ndose en d贸nde *ocurrieron* las emergencias en el pasado. Un sistema de despacho de vanguardia debe ser **proactivo**, posicionando los recursos para satisfacer la demanda *antes* de que ocurra. Esto requiere pasar de un an谩lisis de distribuci贸n a un problema de **pron贸stico de series de tiempo**.

        **Formulaci贸n del Problema:**
        Dado un historial de llamadas de emergencia agregadas por intervalo de tiempo (e.g., por hora o por d铆a), $Y = \{y_1, y_2, \dots, y_T\}$, el objetivo es construir un modelo $f$ que pueda predecir el n煤mero de llamadas en un tiempo futuro $T+h$, es decir, $\hat{y}_{T+h} = f(Y)$.
        """)

        with st.expander("Fundamento Matem谩tico Detallado: Prophet", expanded=True):
            st.markdown(r"""
            **1. Metodolog铆a: Modelo Aditivo Generalizado (GAM)**

            Se propone utilizar **Prophet**, una librer铆a de pron贸stico de Meta AI. Prophet est谩 espec铆ficamente dise帽ado para series de tiempo de negocios que exhiben m煤ltiples estacionalidades y son robustas a datos faltantes y valores at铆picos. Se basa en un **Modelo Aditivo Generalizado (GAM)**, donde las no linealidades se modelan como componentes sumables.

            **2. Formulaci贸n Matem谩tica**

            El modelo Prophet descompone la serie de tiempo $y(t)$ en tres componentes principales m谩s un t茅rmino de error:
            """)
            st.latex(r''' y(t) = g(t) + s(t) + h(t) + \epsilon_t ''')
            st.markdown(r"""
            Donde:
            - **$g(t)$ es la componente de tendencia (Trend):** Modela cambios no peri贸dicos a largo plazo en los datos. Prophet utiliza un modelo de crecimiento lineal por partes (piecewise linear) o log铆stico, lo que le permite detectar y adaptarse autom谩ticamente a los cambios en la tasa de crecimiento de la demanda.
            - **$s(t)$ es la componente de estacionalidad (Seasonality):** Modela cambios peri贸dicos, como patrones diarios, semanales o anuales. Prophet modela la estacionalidad utilizando una **serie de Fourier**, lo que le permite ajustarse a patrones peri贸dicos de formas arbitrarias y suaves. Para un per铆odo $P$ (e.g., $P=7$ para la estacionalidad semanal), la aproximaci贸n es:
            """)
            st.latex(r''' s(t) = \sum_{n=1}^{N} \left(a_n \cos\left(\frac{2\pi nt}{P}\right) + b_n \sin\left(\frac{2\pi nt}{P}\right)\right) ''')
            st.markdown(r"""
            - **$h(t)$ es la componente de feriados y eventos (Holidays):** Modela los efectos de eventos irregulares pero predecibles que no siguen un patr贸n peri贸dico, como d铆as festivos, eventos deportivos importantes o conciertos.
            - **$\epsilon_t$ es el t茅rmino de error:** Representa el ruido idiosincr谩tico que no es capturado por el modelo. Se asume que sigue una distribuci贸n Normal.

            El ajuste del modelo se realiza dentro de un marco **Bayesiano**, lo que permite a Prophet proporcionar no solo un pron贸stico puntual, sino tambi茅n un **intervalo de incertidumbre** que cuantifica la confianza en la predicci贸n.
            
            **3. Justificaci贸n Cient铆fica y Relevancia Operacional**

            - **Robustez y Automatizaci贸n:** A diferencia de los modelos ARIMA cl谩sicos, Prophet no requiere que la serie de tiempo sea estacionaria y es altamente resistente a datos faltantes. Automatiza gran parte de la selecci贸n de hiperpar谩metros, haci茅ndolo ideal para implementaciones a escala.
            - **Manejo de M煤ltiples Estacionalidades:** La demanda de SME tiene fuertes estacionalidades a nivel de hora del d铆a (m谩s accidentes en hora pico), d铆a de la semana (m谩s incidentes relacionados con el ocio los fines de semana) y a帽o (efectos estacionales como la temporada de gripe). El enfoque de series de Fourier de Prophet est谩 dise帽ado precisamente para capturar estas interacciones complejas.
            - **Cuantificaci贸n de la Incertidumbre:** El marco Bayesiano proporciona intervalos de confianza, lo cual es crucial para la toma de decisiones. Un pron贸stico con alta incertidumbre podr铆a llevar a una estrategia de posicionamiento m谩s conservadora, mientras que un pron贸stico de alta confianza podr铆a justificar un posicionamiento m谩s agresivo de los recursos.
            """)
        
        days_to_forecast = st.slider("Par谩metro: Horizonte de Pron贸stico (d铆as)", 7, 90, 30, key="prophet_slider")
        if st.button(" Generar Pron贸stico de Demanda"):
            with st.spinner("Generando pron贸stico... (las ejecuciones son cacheadas por cada valor del slider)"):
                model, forecast, historical_avg, predicted_val = generate_prophet_forecast(days_to_forecast)
            
            if model is not None:
                fig = model.plot(forecast)
                st.pyplot(fig)
                
                st.subheader("An谩lisis de Resultados e Implicaciones Cient铆ficas")
                st.markdown("""
                La gr谩fica muestra los datos hist贸ricos (puntos negros), el pron贸stico del modelo (l铆nea azul) y el intervalo de incertidumbre del 80% (谩rea sombreada). El modelo ha capturado con 茅xito la tendencia y los patrones estacionales (e.g., picos en los fines de semana).
                """)
                col1, col2 = st.columns(2)
                last_forecast_day_str = forecast.iloc[-1]['ds'].strftime('%A')
                col1.metric(f"Promedio Hist贸rico para un {last_forecast_day_str}", f"{historical_avg:.1f} llamadas")
                col2.metric(f"Pron贸stico para el Pr贸ximo {last_forecast_day_str}", f"{predicted_val:.1f} llamadas", delta=f"{predicted_val - historical_avg:.1f}")
                
                st.markdown("""
                **Implicaci贸n Cient铆fica y Operacional:**
                Este enfoque permite una transici贸n fundamental de una **optimizaci贸n reactiva** (basada en promedios hist贸ricos) a una **optimizaci贸n proactiva y anticipatoria**. En lugar de planificar para el "martes promedio", el sistema puede planificar para el "pr贸ximo martes", incorporando tendencias recientes y estacionalidades. Operacionalmente, esto significa que las ambulancias pueden ser reubicadas a zonas de alta demanda *pronosticada* horas antes de que ocurran los picos de llamadas, reduciendo as铆 de manera fundamental los tiempos de respuesta.
                """)
            else:
                st.error("Por favor instale Prophet: pip install prophet")

    def render_simpy_tab(self):
        st.header("Metodolog铆a Propuesta: Simulaci贸n de Sistemas y Aprendizaje por Refuerzo (RL)")
        st.markdown("""
        Los m茅todos de optimizaci贸n cl谩sicos, como el RDSM, son excelentes para la **planificaci贸n estrat茅gica** (d贸nde ubicar las bases a largo plazo). Sin embargo, para la **toma de decisiones t谩cticas en tiempo real** (qu茅 ambulancia enviar a qu茅 llamada *ahora mismo*), se requiere un enfoque m谩s din谩mico. Proponemos un marco de Aprendizaje por Refuerzo (RL), donde un agente de IA aprende una pol铆tica de despacho 贸ptima a trav茅s de la experiencia.

        Para entrenar a un agente de RL sin arriesgar vidas, es esencial construir primero un **"gemelo digital"** del sistema de SME, un entorno de simulaci贸n de alta fidelidad.
        """)

        with st.expander("Fundamento Matem谩tico Detallado: Simulaci贸n de Eventos Discretos y RL", expanded=True):
            st.markdown(r"""
            **1. Metodolog铆a de Simulaci贸n: Teor铆a de Colas y SimPy**

            El sistema de SME se puede modelar como un **sistema de colas M/G/c**.
            - **M (Markoviano):** La llegada de llamadas sigue un **proceso de Poisson**, lo que significa que el tiempo entre llegadas consecutivas sigue una distribuci贸n exponencial.
            - **G (General):** El tiempo de servicio (desde el despacho hasta que la ambulancia vuelve a estar disponible) sigue una distribuci贸n general, ya que depende de muchos factores (tr谩fico, gravedad del incidente, etc.).
            - **c:** Hay $c$ servidores, que corresponde al n煤mero de ambulancias disponibles.

            Utilizamos **SimPy**, una librer铆a de **simulaci贸n de eventos discretos** basada en procesos. A diferencia de las simulaciones por pasos de tiempo fijos, este paradigma solo avanza el tiempo al siguiente evento programado (e.g., "llegada de una llamada", "ambulancia disponible"), lo que lo hace computacionalmente muy eficiente.

            **2. Formulaci贸n Matem谩tica del Aprendizaje por Refuerzo**

            El problema de despacho se formaliza como un **Proceso de Decisi贸n de Markov (MDP)**, definido por la tupla $(\mathcal{S}, \mathcal{A}, P, R, \gamma)$:
            - $\mathcal{S}$ (Espacio de Estados): Una representation del sistema en un momento $t$. Incluye la ubicaci贸n y estado (libre/ocupada) de cada ambulancia, la lista de llamadas en espera con sus prioridades y ubicaciones, y el pron贸stico de demanda a corto plazo.
            - $\mathcal{A}$ (Espacio de Acciones): El conjunto de decisiones que el agente puede tomar. Por ejemplo: `asignar(ambulancia_j, llamada_i)` o `reubicar(ambulancia_k, base_l)`.
            - $P(s'|s,a)$: La funci贸n de probabilidad de transici贸n de estado. Describe la probabilidad de llegar al estado $s'$ si se toma la acci贸n $a$ en el estado $s$. En nuestro caso, esta funci贸n es el **simulador de SimPy**.
            - $R(s,a,s')$: La funci贸n de recompensa. Una se帽al escalar que el agente recibe despu茅s de cada acci贸n. Debe estar dise帽ada para incentivar el comportamiento deseado. Por ejemplo, una recompensa negativa proporcional al tiempo de respuesta: $R = -T_{\text{respuesta}}$.
            - $\gamma \in [0, 1]$: Un factor de descuento que pondera la importancia de las recompensas futuras frente a las inmediatas.

            El objetivo del agente de RL es aprender una **pol铆tica 贸ptima** $\pi^*: \mathcal{S} \to \mathcal{A}$ que maximice la recompensa acumulada esperada (el retorno) a largo plazo:
            """)
            st.latex(r''' \pi^* = \arg\max_{\pi} \mathbb{E} \left[ \sum_{t=0}^{\infty} \gamma^t R_{t+1} \mid \pi \right] ''')
            st.markdown(r"""
            **3. Justificaci贸n Cient铆fica y Relevancia Operacional**

            - **Superaci贸n de las Heur铆sticas Simples:** Las pol铆ticas de despacho humanas a menudo se basan en heur铆sticas simples (e.g., "enviar siempre la unidad m谩s cercana"). El RL permite al agente aprender **pol铆ticas complejas y no intuitivas**. Por ejemplo, podr铆a aprender a no enviar la ambulancia m谩s cercana a una llamada no cr铆tica si esa ambulancia es la 煤nica que cubre una zona con alta probabilidad de una llamada card铆aca inminente, seg煤n el pron贸stico de Prophet.
            - **Adaptabilidad Din谩mica:** Un agente de RL puede adaptarse a condiciones cambiantes. Si se produce un gran accidente de tr谩fico, el estado del sistema cambia drasticamente, y la pol铆tica aprendida puede tomar decisiones que tengan en cuenta esta nueva realidad, algo que un plan de optimizaci贸n est谩tico no puede hacer.
            """)

        st.header("Demostraci贸n: Simulaci贸n de un Sistema con Prioridad")
        num_ambulances = st.slider("Par谩metro: N煤mero de Ambulancias (Servidores, c)", 1, 10, 3, key="simpy_slider_1")
        avg_call_interval = st.slider("Par谩metro: Tiempo Promedio Entre Llamadas (1/位)", 5, 60, 20, key="simpy_slider_2")
        
        @st.cache_data
        def run_dispatch_simulation(ambulances, interval):
            """Encapsulates the entire SimPy simulation for stability with Streamlit."""
            try:
                import simpy
            except ImportError:
                return "SimPy no instalado", "SimPy no instalado"
            
            wait_times_priority = []
            wait_times_standard = []
            
            def call_process(env, fleet, is_priority):
                arrival_time = env.now
                priority_level = 1 if is_priority else 2
                with fleet.request(priority=priority_level) as request:
                    yield request
                    wait_time = env.now - arrival_time
                    if is_priority: wait_times_priority.append(wait_time)
                    else: wait_times_standard.append(wait_time)
                    yield env.timeout(random.uniform(20, 40))

            def call_generator(env, fleet, interval):
                for _ in range(500):
                    is_priority_call = random.random() < 0.2
                    env.process(call_process(env, fleet, is_priority_call))
                    yield env.timeout(random.expovariate(1.0 / interval))
            
            env = simpy.Environment()
            fleet = simpy.PriorityResource(env, capacity=ambulances)
            env.process(call_generator(env, fleet, interval))
            env.run()
            return np.mean(wait_times_priority) if wait_times_priority else 0, np.mean(wait_times_standard) if wait_times_standard else 0

        if st.button(" Ejecutar Simulaci贸n de Sistema de Colas con Prioridad"):
            with st.spinner("Simulando... (las ejecuciones son cacheadas por cada combinaci贸n de par谩metros)"):
                priority_wait, standard_wait = run_dispatch_simulation(num_ambulances, avg_call_interval)
                
                if isinstance(priority_wait, str):
                    st.error("Por favor instale SimPy: pip install simpy")
                else:
                    st.subheader("Resultados de la Simulaci贸n")
                    col1, col2 = st.columns(2)
                    col1.metric("Tiempo de Espera Promedio (Llamadas Prioritarias)", f"{priority_wait:.2f} min")
                    col2.metric("Tiempo de Espera Promedio (Llamadas Est谩ndar)", f"{standard_wait:.2f} min")

                    with st.expander("An谩lisis de Resultados e Implicaciones Cient铆ficas", expanded=True):
                        st.markdown("""
                        **An谩lisis de la Simulaci贸n:**
                        La simulaci贸n utiliza una **cola de prioridad**, un modelo m谩s realista que un simple sistema "primero en llegar, primero en ser servido". Los resultados muestran que, incluso con recursos limitados, el sistema puede mantener un tiempo de espera muy bajo para las llamadas cr铆ticas, a costa de un tiempo de espera mayor para las no cr铆ticas. Este es el comportamiento deseado y valida que el simulador captura din谩micas de sistemas realistas.
                        
                        **Implicaciones para el Aprendizaje por Refuerzo:**
                        Este entorno simulado es la pieza clave que permite la aplicaci贸n de algoritmos de RL. La funci贸n de recompensa del agente se dise帽ar铆a para minimizar una combinaci贸n ponderada de estos tiempos de espera:
                        """)
                        st.latex(r''' R = - (w_p \cdot \overline{T}_{\text{espera, prioridad}} + w_s \cdot \overline{T}_{\text{espera, est谩ndar}}) ''')
                        st.markdown("""
                        donde $w_p \gg w_s$. Un agente de RL entrenado en esta simulaci贸n aprender铆a una pol铆tica de despacho que va m谩s all谩 de la simple prioridad. Podr铆a aprender a **reservar estrat茅gicamente una ambulancia** en una zona de alta probabilidad de llamadas prioritarias, rechazando temporalmente atender una llamada est谩ndar en otro lugar, si su modelo interno predice que hacerlo maximizar谩 la recompensa a largo plazo. Esta capacidad de tomar decisiones estrat茅gicas y dependientes del contexto es lo que diferencia al RL de las pol铆ticas heur铆sticas fijas.
                        """)

# ==============================================================================
# 5. MAIN APPLICATION ROUTER
# ==============================================================================
def main():
    """Main function to route to the correct page."""
    render_sidebar_info()
    pages = {
        "Resumen de la Tesis": ThesisSummaryPage("Resumen de la Tesis", ""),
        "Calibraci贸n del Modelo de Tiempos": TimeCorrectionPage("Calibraci贸n del Modelo", "憋"),
        "Clustering de Demanda": ClusteringPage("Clustering de Demanda", ""),
        "Optimizaci贸n de Ubicaciones": OptimizationPage("Optimizaci贸n de Ubicaciones", ""),
        "Evoluci贸n del Sistema con IA Avanzada": AIEvolutionPage("Evoluci贸n con IA", "")
    }
    selected_page_title = st.sidebar.radio("Seleccione un M贸dulo Anal铆tico:", list(pages.keys()))
    page = pages[selected_page_title]
    page.render()
    render_mathematical_foundations()

if __name__ == "__main__":
    main()
